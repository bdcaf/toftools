\section{Introduction}

In this paper a modern work flow is shown.

Currently most user work with the TOF data analysis tool
\citep{Muller2013a} provided by Ionicon.  It is a convenient tool;
unfortunately it does not fit every work flow.

\subsection{possible issues}
\paragraph{Data issue}

One big issue for me is actually the data file.  It contains not only
the measurement data --- but also part of the evaluation.  Specifically
the mass calibration, and derived from it peak areas.   Although these
are initially written at measurement time, during a typical evaluation
both will be recreated.  

There are a number of methods for mass calibration available, and more
important different points of reference may be used; also it is possible
to select whether the calibration is performed on the sum spectrum or
separately for each buf. 

The same is true for integration.  One can choose from several options
how to integrate peaks, and also needs to define which peaks will be
integrated. 

The real issue being the data file gets modified by viewing it.  This
has two important implications: 1st as the file gets rewritten any
hardware or software error may destroy the data.  2nd a problem of
reproducibility; a user might change values inside the data file by
opening it --- or consider two persons working on the same file may end
up with different results.

In our work group we believe that measurement data should be stored
read-only and may not be altered after capture.


\paragraph{missing batch processing}

For larger measurement campaigns a reproducible work flow is essential.
This means that every measurement must be processed exactly the same
way.  This sounds like a non-issue, but in practice is difficult to
achieve.  Usually during the course of any campaign issues appear which
require adjusting the evaluation.

Doing this manually one file at a time is not only laborious, but also a
great risk of error. A tool that will allow for reevaluation of a large
set of measurements with clearly spelled out parameter can help in this
process.


\paragraph{modularity}

Making the evaluation modular has two great benefits.  For one users can
replace any module with one that suits their need; especially as modules
can be created directly by the user. Second the intermediate
results between modules may be stored for reuse.  The latter requires
dependency tracking, but we will show how this can easily be achieved.

\paragraph{Mass calibration}

I am not very happy with the way mass calibration is currently
performed.  For one it uses the buf as smallest unit, although this
averages over a number of scans.  The next issue is that peaks are
actually fit to a curve; This does not seem reasonable for noisy peaks.
Third when plotted the coefficients of the calibration function do not
follow a smooth curve, my personal interpretation is that here noise
in the estimation of these coefficients manifests.  Lastly I am worried
that the whole calibration hangs on just a few masses.  

\subsection{outline}

The mass calibration actually performs two tasks. One it corrects for
drifts in the flight time; and two it translates flight time to mass.

It is difficult to correct for drifts, but recently a family of
corrections were introduced under the name of \emph{time warping}.  For
TOF spectra the parametric time warp \citep{Eilers2004a} is appropriate.

Once the drift has been eliminated mass calibration can be performed on
recalculated sum spectra, which contain peaks that may be absent from
individual spectra.

For integration we provide modules to integrate predefined masses as
well as automatic detection of peaks in the sum spectrum and calculating
their traces. 
